from vggblock import VGGBlock
import torch.nn as nn
import torch
from speechbrain.lobes.models.transformer.TransformerASR import (
    get_key_padding_mask, get_lookahead_mask, 
    length_to_mask
)
from speechbrain.lobes.models.transformer.Transformer import (
    TransformerEncoder, TransformerDecoder, NormalizedEmbedding, get_lookahead_mask
)
from speechbrain.nnet.containers import ModuleList
from speechbrain.nnet.linear import Linear
from fairseq.modules import (
    LinearizedConvolution
)
import math

def make_transformer_src_tgt_masks(
    src,
    tgt=None,
    wav_len=None,
    pad_idx=0,
    causal: bool = False,
    dynchunktrain_config = None,
):
    """This function generates masks for training the transformer model,
    opinionated for an ASR context with encoding masks and, optionally, decoding
    masks (if specifying `tgt`).

    Arguments
    ---------
    src : torch.Tensor
        The sequence to the encoder (required).
    tgt : torch.Tensor
        The sequence to the decoder.
    wav_len : torch.Tensor
        The lengths of the inputs.
    pad_idx : int
        The index for <pad> token (default=0).
    causal: bool
        Whether strict causality shall be used. See `make_asr_src_mask`
    dynchunktrain_config: DynChunkTrainConfig, optional
        Dynamic Chunk Training configuration. See `make_asr_src_mask`

    Returns
    -------
    src_key_padding_mask : torch.Tensor
        Key padding mask for ignoring padding
    tgt_key_padding_mask : torch.Tensor
        Key padding mask for ignoring padding
    src_mask : torch.Tensor
        Mask for ignoring invalid (e.g. future) timesteps
    tgt_mask : torch.Tensor
        Mask for ignoring invalid (e.g. future) timesteps
    """
    src_key_padding_mask = None

    # mask out audio beyond the length of audio for each batch
    if wav_len is not None:
        abs_len = torch.round(wav_len * src.shape[1])
        src_key_padding_mask = ~length_to_mask(abs_len).bool()

    # mask out the source
    src_mask = make_transformer_src_mask(
        src, causal=causal, dynchunktrain_config=dynchunktrain_config
    )

    # If no decoder in the transformer...
    if tgt is not None:
        tgt_key_padding_mask = get_key_padding_mask(tgt, pad_idx=pad_idx)
        tgt_mask = get_lookahead_mask(tgt)
    else:
        tgt_key_padding_mask = None
        tgt_mask = None

    return src_key_padding_mask, tgt_key_padding_mask, src_mask, tgt_mask

def make_transformer_src_mask(
    src: torch.Tensor,
    causal: bool = False,
    dynchunktrain_config= None,
) :
    """Prepare the source transformer mask that restricts which frames can
    attend to which frames depending on causal or other simple restricted
    attention methods.

    Arguments
    ---------
    src: torch.Tensor
        The source tensor to build a mask from. The contents of the tensor are
        not actually used currently; only its shape and other metadata (e.g.
        device).
    causal: bool
        Whether strict causality shall be used. Frames will not be able to
        attend to any future frame.
    dynchunktrain_config: DynChunkTrainConfig, optional
        Dynamic Chunk Training configuration. This implements a simple form of
        chunkwise attention. Incompatible with `causal`.

    Returns
    -------
    torch.Tensor
        A boolean mask Tensor of shape (timesteps, timesteps).
    """
    if causal:
        assert dynchunktrain_config is None
        return get_lookahead_mask(src)

    if dynchunktrain_config is None:
        return

    # The following is not really the sole source used to implement this,
    # but it helps introduce the concept.
    # ref: Unified Streaming and Non-streaming Two-pass End-to-end Model for Speech Recognition
    # https://arxiv.org/pdf/2012.05481.pdf
    timesteps = src.size(1)

    # Mask the future at the right of each chunk
    chunk_size = dynchunktrain_config.chunk_size
    num_chunks = timesteps // chunk_size
    timestep_idx = torch.arange(timesteps, device=src.device)
    mask_idx = torch.arange(
        chunk_size, chunk_size * (num_chunks + 2), chunk_size, device=src.device
    ).repeat_interleave(chunk_size)[:timesteps]
    src_mask = timestep_idx[None] >= mask_idx[:, None]

    # Mask the past at the left of each chunk (accounting for left context)
    # only relevant if using left context
    if not dynchunktrain_config.is_infinite_left_context():
        num_left_chunks = dynchunktrain_config.left_context_size
        mask_idx -= chunk_size * (num_left_chunks + 1)
        src_mask += timestep_idx[None] < mask_idx[:, None]

    return src_mask


def compute_wav_len_from_src(src, threshold=0.1):
    """T√≠nh to√°n wav_len d·ª±a tr√™n src b·∫±ng c√°ch ƒë·∫øm s·ªë frame c√≥ nƒÉng l∆∞·ª£ng v∆∞·ª£t ng∆∞·ª°ng."""
    batch_size, max_length, _, _ = src.shape  # (B, T, C, Feat)

    # T√≠nh t·ªïng nƒÉng l∆∞·ª£ng tr√™n m·ªói frame (B, T)
    energy = src.abs().sum(dim=(2, 3))  # T·ªïng qua C v√† Feat ‚Üí ch·ªâ c√≤n (B, T)

    # ƒê·∫øm s·ªë frame c√≥ nƒÉng l∆∞·ª£ng l·ªõn h∆°n threshold
    actual_length = (energy > threshold).sum(dim=1).float()  # (B,)

    # Chu·∫©n h√≥a v·ªÅ kho·∫£ng [0, 1]
    wav_len = actual_length / max_length
    return wav_len

def LinearizedConv1d(in_channels, out_channels, kernel_size, dropout=0, **kwargs):
    """Weight-normalized Conv1d layer optimized for decoding"""
    m = LinearizedConvolution(in_channels, out_channels, kernel_size, **kwargs)
    std = math.sqrt((4 * (1.0 - dropout)) / (m.kernel_size[0] * in_channels))
    nn.init.normal_(m.weight, mean=0, std=std)
    nn.init.constant_(m.bias, 0)
    return nn.utils.weight_norm(m, dim=2)

class VGGTransformerEncoderDecoder(nn.Module):
    def __init__(
        self, 
        input_feat_per_channel, 
        vggblock_config_enc, 
        vggblock_config_dec, 
        in_channels=1, 
        input_size=1280, 
        d_model=512, 
        dropout=0.1, 
        nhead=8, 
        num_encoder_layers=6, 
        num_decoder_layers=6,
        d_ffn=2048, 
        activation="relu", 
        normalize_before=True, 
        attention_type="self-attention", 
        causal=False, 
        kdim=None, 
        vdim=None, 
        layerdrop_prob=0.1, 
        output_hidden_states=False,
        tgt_vocab=1000,
        decoder_kdim=None,
        decoder_vdim=None
    ):
        super().__init__()

        self.conv_layers_dec_enc = nn.ModuleList()
        self.conv_layers_dec_dec = nn.ModuleList()
        self.in_channels = in_channels
        self.input_dim = input_feat_per_channel
        self.pooling_kernel_sizes = []
        
        # ====== Encoder VGG Blocks ======
        for _, config in enumerate(vggblock_config_enc):
            out_channels, conv_kernel_size, pooling_kernel_size, num_conv_layers, layer_norm = config # (32, 3, 1, 1, True)
            self.conv_layers_dec_enc.append(
                VGGBlock(
                    in_channels, out_channels, conv_kernel_size, pooling_kernel_size, 
                    num_conv_layers, input_dim=input_feat_per_channel, layer_norm=layer_norm
                )
            )
            in_channels = out_channels
            input_feat_per_channel = self.conv_layers_dec_enc[-1].output_dim

        # ====== Transformer Encoder ======
        self.custom_src_module = ModuleList(
            Linear(input_size=input_size, n_neurons=d_model, bias=True, combine_dims=False),
            torch.nn.Dropout(dropout),
        )

        self.encoder = TransformerEncoder(
            nhead=nhead, num_layers=num_encoder_layers, d_ffn=d_ffn, d_model=d_model,
            dropout=dropout, activation=activation, normalize_before=normalize_before,
            causal=causal, attention_type=attention_type, kdim=kdim, vdim=vdim,
            layerdrop_prob=layerdrop_prob,
        )


        # ====== Decoder VGG Blocks ======
        self.conv_layers_dec = nn.ModuleList()
        for i in range(len(vggblock_config_dec)):
            out_channels, kernel_size, layer_norm = vggblock_config_dec[i]
            if i == 0:
                conv_layer = LinearizedConv1d(
                    d_model, out_channels, kernel_size, padding=kernel_size - 1
                )
            else:
                conv_layer = LinearizedConv1d(
                    vggblock_config_dec[i - 1][0],
                    out_channels,
                    kernel_size,
                    padding=kernel_size - 1,
                )
            self.conv_layers_dec.append(conv_layer)
            if layer_norm:
                self.conv_layers_dec.append(nn.LayerNorm(out_channels))
            self.conv_layers_dec.append(nn.ReLU())

        # ====== Transformer Decoder ======
        self.custom_tgt_module = ModuleList(NormalizedEmbedding(d_model, tgt_vocab))

        self.decoder = TransformerDecoder(
            num_layers=num_decoder_layers, nhead=nhead, d_ffn=d_ffn, d_model=d_model,
            dropout=dropout, activation=activation, normalize_before=normalize_before,
            causal=True, attention_type="regularMHA", kdim=decoder_kdim, vdim=decoder_vdim,
        )

    def forward(self, src, tgt, wav_len, pad_idx=0):
        print(f"üîπ [STEP 1] Input src.shape: {src.shape}")  # (B, T, C, Feat)
        print(f"üîπ [STEP 1] Input tgt.shape: {tgt.shape}")  # (B, T)
        print(f"üîπ [STEP 1] Input wav_len.shape: {wav_len.shape}")  # (B,)

        # bsz, max_seq_len, _ = src_tokens.size()
        # x = src_tokens.view(bsz, max_seq_len, self.in_channels, self.input_dim)
        # x = x.transpose(1, 2).contiguous()

        src = src.unsqueeze(2)
        src = src.transpose(1, 2).contiguous()
        


        # print("input dim : ", self.input_dim)
        # bsz, max_seq_len, _ = src.size()
        # src = src.view(bsz, max_seq_len, self.in_channels, self.input_dim)
        # src = src.transpose(1, 2).contiguous() # B C T F


        # # N·∫øu src c√≥ 4 chi·ªÅu (B, T, C, F), c·∫ßn ho√°n v·ªã l·∫°i
        # if src.ndim == 4:
        #     src = src.transpose(1, 2).contiguous()  # (B, T, C, F) -> (B, C, T, F)
        #     print(f"üîπ [STEP 2] After transpose src.shape: {src.shape}")
        
        for layer_idx, layer in enumerate(self.conv_layers_dec_enc):
            src = layer(src)
            print(f"üîπ [STEP 3] After VGGBlock {layer_idx} (Encoder), src.shape: {src.shape}")

        src = src.permute(0, 2, 1, 3).contiguous()
        print(f"üîπ [STEP 4] After permute src.shape: {src.shape}")

        bsz, time, c, feat = src.shape
        src = src.view(bsz, time, c * feat)
        print(f"üîπ [STEP 5] Final src.shape before Transformer: {src.shape}")

        abs_len = torch.clamp(torch.round(wav_len * time).long(), max=time)
        print(f"üîπ [STEP 6] abs_len: {abs_len}")

        src_key_padding_mask, tgt_key_padding_mask, src_mask, tgt_mask = make_transformer_src_tgt_masks(
            src, tgt, wav_len, causal=False, pad_idx=pad_idx
        )

        src = self.custom_src_module(src)
        print(f"üîπ [STEP 7] After custom_src_module src.shape: {src.shape}")

        encoder_out, _ = self.encoder(
            src=src, src_mask=src_mask, src_key_padding_mask=src_key_padding_mask, pos_embs=None
        )

        print(f"üîπ [STEP 8] Transformer Encoder output.shape: {encoder_out.shape}")

        tgt = self.custom_tgt_module(tgt)
        print(f"üîπ [STEP 9] After tgt embedding tgt.shape: {tgt.shape}") # B x T x C 
        tgt = tgt.transpose(0,1) # b t c -> t b c 


        for layer in self.conv_layers_dec:
            tgt = layer(tgt)
        
        print(f"After reshape, tgt.shape: {tgt.shape}")
        tgt = tgt.transpose(0,1)
        # t b c -> btc
        decoder_out, _, _ = self.decoder(
            tgt=tgt, memory=encoder_out, memory_mask=None, tgt_mask=tgt_mask,
            tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=src_key_padding_mask
        )

        print(f"üîπ [STEP 10] Transformer Decoder output.shape: {decoder_out.shape}")

        return encoder_out, decoder_out

    @torch.no_grad()
    def decode(self, tgt, encoder_out, enc_len=None):
        """This method implements a decoding step for the transformer model.

        Arguments
        ---------
        tgt : torch.Tensor
            The sequence to the decoder.
        encoder_out : torch.Tensor
            Hidden output of the encoder.
        enc_len : torch.LongTensor
            The actual length of encoder states.

        Returns
        -------
        prediction
        """
        tgt_mask = get_lookahead_mask(tgt)
        src_key_padding_mask = None
        if enc_len is not None:
            src_key_padding_mask = (1 - length_to_mask(enc_len)).bool()
        
        tgt = self.custom_tgt_module(tgt)

        tgt = tgt.transpose(0,1) # b t c -> t b c 


        for layer in self.conv_layers_dec:
            tgt = layer(tgt)
        
        print(f"After reshape, tgt.shape: {tgt.shape}")
        tgt = tgt.transpose(0,1)

        pos_embs_target = None
        pos_embs_encoder = None

        prediction, self_attns, multihead_attns = self.decoder(
            tgt,
            encoder_out,
            tgt_mask=tgt_mask,
            memory_key_padding_mask=src_key_padding_mask,
            pos_embs_tgt=pos_embs_target,
            pos_embs_src=pos_embs_encoder,
        )
        return prediction, multihead_attns[-1]

    def encode(
        self,
        src,
        wav_len=None,
        pad_idx=0,
        dynchunktrain_config = None,
    ):
        """
        Encoder forward pass

        Arguments
        ---------
        src : torch.Tensor
            The sequence to the encoder.
        wav_len : torch.Tensor, optional
            Torch Tensor of shape (batch, ) containing the relative length to padded length for each example.
        pad_idx : int
            The index used for padding.
        dynchunktrain_config : DynChunkTrainConfig
            Dynamic chunking config.

        Returns
        -------
        encoder_out : torch.Tensor
        """
        # reshape the src vector to [Batch, Time, Fea] if a 4d vector is given

        
        src = src.unsqueeze(2)
        print(src.shape)
        src = src.transpose(1, 2).contiguous()
        # bsz, max_seq_len, _ = src.size()
        # src = src.view(bsz, max_seq_len, self.in_channels, self.input_dim)
        # src = src.transpose(1, 2).contiguous() # B C T F
        
        for layer_idx, layer in enumerate(self.conv_layers_dec_enc):
            src = layer(src)
            print(f"üîπ [STEP 3] After VGGBlock {layer_idx} (Encoder), src.shape: {src.shape}")

        src = src.permute(0, 2, 1, 3).contiguous()
        print(f"üîπ [STEP 4] After permute src.shape: {src.shape}")

        bsz, time, c, feat = src.shape
        src = src.view(bsz, time, c * feat)
        print(f"üîπ [STEP 5] Final src.shape before Transformer: {src.shape}")

        abs_len = torch.clamp(torch.round(wav_len * time).long(), max=time)
        print(f"üîπ [STEP 6] abs_len: {abs_len}")

        src_key_padding_mask, tgt_key_padding_mask, src_mask, tgt_mask = make_transformer_src_tgt_masks(
            src, None, wav_len, causal=False, pad_idx=pad_idx
        )

        src = self.custom_src_module(src)
        print(f"üîπ [STEP 7] After custom_src_module src.shape: {src.shape}")

        encoder_out, _ = self.encoder(
            src=src, src_mask=src_mask, src_key_padding_mask=src_key_padding_mask, pos_embs=None
        )
        return encoder_out




# C·∫•u h√¨nh VGG Block Encoder
vggblock_config_enc = [
    (64, 3, 2, 2, True),  # (out_channels, conv_kernel, pool_kernel, num_layers, layer_norm)
    (128, 3, 2, 2, True)
]

# C·∫•u h√¨nh VGG Block Decoder
vggblock_config_dec = [
    (256, 3, True),
    (256, 3, True)
]

# C·∫•u h√¨nh Transformer
input_size = 2560  # S·ªë feature ƒë·∫ßu v√†o sau VGG
d_model = 256  # S·ªë chi·ªÅu c·ªßa embedding trong Transformer
num_decoder_layers = 6  # S·ªë t·∫ßng Decoder
tgt_vocab = 1000  # Gi·∫£ ƒë·ªãnh vocab c√≥ 1000 t·ª´

# Kh·ªüi t·∫°o m√¥ h√¨nh
device = "cuda" if torch.cuda.is_available() else "cpu"
print(device)
model = VGGTransformerEncoderDecoder(
    input_feat_per_channel=80,
    vggblock_config_enc=vggblock_config_enc,  
    vggblock_config_dec=vggblock_config_dec,  
    in_channels=1,
    input_size=input_size,
    d_model=d_model,
    dropout=0.1,
    nhead=8,
    num_encoder_layers=6,
    num_decoder_layers=num_decoder_layers,  
    d_ffn=2048,
    activation=nn.ReLU,
    normalize_before=False,
    attention_type="regularMHA",
    causal=False,
    kdim=None,
    vdim=None,
    layerdrop_prob=0.1,
    output_hidden_states=False,
    tgt_vocab=tgt_vocab,  
).to(device)

# ====== D·ªØ li·ªáu gi·∫£ l·∫≠p m·ªõi ======
batch_size = 4
time_steps = 2050  # S·ªë khung th·ªùi gian t·ªëi ƒëa
features = 80  # S·ªë feature m·ªói frame

# T·∫°o src ng·∫´u nhi√™n v·ªõi shape (B, T, F)
src = torch.randn(batch_size, time_steps, features).to(device)

# T·∫°o tgt ng·∫´u nhi√™n (B, T) v·ªõi vocab size 1000
tgt = torch.randint(0, tgt_vocab, (batch_size, 50)).to(device)

# T√≠nh wav_len t·ª´ src
wav_len = compute_wav_len_from_src(src.unsqueeze(2), threshold=0.1)  # C·∫ßn th√™m dim C=1 t·∫°m th·ªùi ƒë·ªÉ t√≠nh nƒÉng l∆∞·ª£ng
print(f"‚úÖ Generated wav_len: {wav_len}")  # Debug gi√° tr·ªã wav_len

# ====== Ch·∫°y th·ª≠ model ======
output = model(src, tgt, wav_len)

# ====== In k·∫øt qu·∫£ ======
print(f"‚úÖ Output shapes: {[o.shape if o is not None else 'None' for o in output]}")  


# ====== Ch·∫°y th·ª≠ ENCODER ======
print("\nüîπ [TEST] Running Encoder...")
encoder_out = model.encode(src, wav_len)

# Debug output
print(f"‚úÖ Encoder output.shape: {encoder_out.shape}")


# ====== Ch·∫°y th·ª≠ DECODER ======
print("\nüîπ [TEST] Running Decoder...")

# Gi·∫£ l·∫≠p ƒë·∫ßu v√†o cho decoder v·ªõi c√¢u c√≥ 10 t·ª´
tgt_test = torch.randint(0, tgt_vocab, (batch_size, 10)).to(device)

# Ch·∫°y decode
decoder_out, attn_weights = model.decode(tgt_test, encoder_out)

# Debug output
print(f"‚úÖ Decoder output.shape: {decoder_out.shape}")
print(f"‚úÖ Attention Weights.shape: {attn_weights.shape}")

